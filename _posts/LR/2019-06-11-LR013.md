---
title : "[LR#013] SEQ2SEQ-VIS : A Visual Debugging Tool for Sequence-to-Sequence Models"
category :
  - LR
tag :
  - Visualization
  - seq2seq
  - tool
sidebar:
  nav: sidebar-youtube
use_math : true
header:
  overlay_image : "http://www.lighting-inspiration.com/wp-content/uploads/2015/08/Lighting-Inspiration.com_Rohinni-Lightpaper1.jpg"
  overlay_color : "#E0BBE4"
---
딥러닝 시각화 시리즈 1

딥러닝을 이해하기 힘든 이유 중 하나는 모델에 대한 이해가 어렵기 때문입니다. 이런 모델에 대한 이해를 높이기 위해서는 어떤 방법이 있을까요?

또한 모델에 대한 이해가 있더라도 막상 어디서 에러가 발생한지 찾기가 어렵습니다.

저는 이런 문제의 솔루션 중에서는 시각화하여 보여주는 것이 가장 좋다고 생각합니다.

이번 논문은 Seq2Seq 모델 시각화 툴을 만든 논문이자 코드입니다. 논문 제목도 **SEQ2SEQ-VIS : A Visual Debugging Tool for
Sequence-to-Sequence Models** 입니다.

시각화된 결과도 이쁘지만, 문제 제기, 목표 설정, task 설정 등 생각을 확장하는 과정이 매력적인 논문이었습니다.

- **paper** : https://arxiv.org/pdf/1804.09299.pdf
- **code** : https://github.com/HendrikStrobelt/Seq2Seq-Vis
- **website** : https://seq2seq-vis.io/

## 초록으로 읽기

<figure>
<img src = "https://i.imgur.com/mayUsrP.png">
<figcaption>
“our tool helps to find errors in seq2seq models
using visual analysis methods.”문장을 독일어로 번역하는 과정. seq2seq은 사전에 없어 unknown을 의미하는 unk로 표현된다. </figcaption>
</figure>

Neural Sequence-to-Sequence 모델은 많은 시퀀스 예측 작업에 대해 정확하고 견고하며 텍스트의 자동 번역을 위한 표준 방법이되었습니다.

이 모델은 원본 시퀀스를 벡터 공간으로 인코딩한 다음 새로운 시퀀스로 디코딩하는 5 단계 블랙박스 프로세스에서 작동합니다.

이 과정은 현재 표준이긴하지만, 많은 딥러닝 방법을 이해하거나 디버깅하기가 매우 어렵습니다.

이 논문에서는 번역 프로세스의 각 단계를 통해 훈련된 sequence-to-sequence 모델과 상호 작용이 가능한 시각적 분석 도구를 제시합니다.

목표는 학습된 패턴을 식별하고 모델에서 오류를 감지하는 것입니다.

우리는 real-world 대규모 시퀀스 간 (Sequence-to-sequence) 사용 사례를 통해 툴의 유용성을 입증합니다.

## 더 읽어보기

### SEQ2SEQ-VIS의 3 goals

논문은 총 3가지의 핵심 목표이자 의의를 가집니다.

1. **G1** Examine Model Decisions : 모델 파이프라인에서 각 단계에 대한 모델 오류를 이해할 수 있으며, 설명할 수 있다.

2. **G2** Decisions to Samples : 모델의 결과물에 대한 출처를 훈련 데이터와 연관시켜 설명할 수 있다.

3. **G3** Test Alternative Decisions : 모델 내부를 손쉽게 조작하고, 개입이 용이해진다.

### Seq2Seq 모델과 Attention, 5단계

<figure>
<img src = "https://i.imgur.com/h4Oecai.png">
<figcaption> seq2seq 번역 프로세스 </figcaption>
</figure>

seq2seq 번역 프로세스에서 총 5단계를 거칩니다.

1. Encoder
2. Decoder
3. Attention
4. Prediction
5. Search

attention과 seq2seq 번역 모델에서는 이에 대한 좋은 설명이 있어 블로그를 추천합니다.

[lovit님의 Attention mechanism in NLP. From seq2seq + attention to BERT](https://lovit.github.io/machine%20learning/2019/03/17/attention_in_nlp/)

### 사례 분석 : 번역 디버깅

- 다음은 (가정) Encoder에서 생성되는 에러입니다.

- 다음은 (가정) Decoder에서 생성되는 에러입니다.

- 다음은 (가정) Attention, Prediction, Search에서 나오는 에러입니다.

<figure class = "third">
<img src = "https://i.imgur.com/v93NzQL.png">
<img src = "https://i.imgur.com/IVXtkaJ.png">
<img src = "https://i.imgur.com/cOc0Uzi.png">
<figcaption>Hypothesis : 3 Error</figcaption>
</figure>

이후 부분적으로 수정이 가능한 해결책을고민해보았고, 아래는 테스팅해본 결과입니다.

<figure>
<img src = "https://i.imgur.com/bG89md7.png">
<figcaption>Testing</figcaption>
</figure>


### Goals and Tasks

목표는 위의 G1, G2, G3와 같고 각각에 대한 작업은 다음과 같습니다.

**T1** : 5단계에서 공통적으로 사용할 수 있는 모델 [G1]

**T2** : 학습된 표현의 높은 수준의 시각화를 위하여 잠재 벡터 시퀀스의 시간 경과에 따른 상태 진행을 시각화 [G1]

**T3** : 오류 식별 및 훈련 조정을 용이하게 하기 위해 대규모 훈련 예제 데이터베이스에서 쿼리를 통해 생성된 잠재 벡터와 그에 가장 가까운 이웃을 호출 [G2]

**T4** : 유사한 결정을 생성하고 비교하여 선택할 수 있도록 만듬. [G1, G3]

**T5** : 번역, 요약, 생성과 같은 많은 seq2seq 문제에 대해 유사한 프론트엔드를 사용하기 위해 일반적이고 일관된 인터페이스 생성 [G1, G2, G3]

### Design of Seq2Seq-Vis

전체적인 그림

<figure class = "third">
<img src = "https://i.imgur.com/woxTwWJ.png">
<img src = "https://i.imgur.com/zUw7Afu.png">
<img src = "https://i.imgur.com/CiVAXxW.png">
<figcaption>Overview</figcaption>
</figure>

### Use Cases

<figure class = "third">
<img src = "https://i.imgur.com/cw0B4gv.png">
<img src = "https://i.imgur.com/BW6SqFV.png">
<img src = "https://i.imgur.com/BEnigoa.png">
<figcaption>Use Cases</figcaption>
</figure>

Date Conversion, Abstractive Summarization, Machine Translation 사용 예시입니다.
