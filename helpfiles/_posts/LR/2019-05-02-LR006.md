---
title : "[LR#006] Looking to Listen at the Cocktail Party"
category :
  - LR
tag :
  - sketch
  - Deeplearning
  - colorization
sidebar:
  nav: sidebar-youtube
use_math : true
header:
  overlay_image : "http://www.lighting-inspiration.com/wp-content/uploads/2015/08/Lighting-Inspiration.com_Rohinni-Lightpaper1.jpg"
  overlay_color : "#E0BBE4"
---
칵테일 효과를 AI로?

보통 소음이 많은 환경에서 목소리를 알아 듣기 위해서는 어떤 작업이 필요할까요?
사람의 얼굴을 보며 입모양을 보면 어느 정도 들을 수 있습니다. 이런 현상을 [칵테일 파티 효과](https://ko.wikipedia.org/wiki/%EC%B9%B5%ED%85%8C%EC%9D%BC_%ED%8C%8C%ED%8B%B0_%ED%9A%A8%EA%B3%BC)라고 합니다.

이런 작업을 인공지능을 통해 해결한 구글러분들이 있습니다. 이번 paper 제목은 **Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation** 입니다.

제목부터 기대가 되는 제목입니다. Google Research에서 2018년 4월에 발표한 paper입니다.
전문성부터 재미까지 다 연구하는 구글의 능력은 대단합니다.

구글의 세계정복을 막기 위해 저희도 열심히 공부합시다..!

- paper : [https://arxiv.org/abs/1804.03619](https://arxiv.org/abs/1804.03619)
- webpage : [https://looking-to-listen.github.io/](https://looking-to-listen.github.io/)
- youtube : [https://youtu.be/rVQVAPiJWKU](https://youtu.be/rVQVAPiJWKU)
- dataset : [https://looking-to-listen.github.io/avspeech/](https://looking-to-listen.github.io/avspeech/)

## 초록으로 읽기

본 논문에서는 다른 화자 또는 주변 소음으로부터 단일 음성 신호를 분리하기 위한 joint audio-visual 모델을 제시합니다.

이는 오디오만 입력으로 사용하여 해결하는 것은 매우 어렵고, 분리된 음성 신호(목소리)를 화자와 연결 하기도 어렵습니다.

이 논문에서는 이 작업을 해결하기 위해 시각 및 청각 신호를 통합 한 심층 네트워크 기반 모델을 제시합니다. 시각 요소는 장면의 원하는 화자의 소리를 "**집중**"하고, 음성 분리 품질을 향상시키는 데 사용됩니다.

이 모델을 교육하기 위해 웹에서 수천 시간 분량의 비디오 세그먼트로 구성된 새로운 데이터 세트 인 AVSpeech을 소개합니다. (Dataset 공개)

우리는 고전적인 언어 분리 작업 뿐만 아니라 열띤 인터뷰, 시끄러운 바, 소리 지르는 어린이를 포함하는 실제에 있을 법한 상황에서 성능을 보여줍니다. 여기서는 사용자가 영상에서 음성을 분리하기를 원하는 화자의 얼굴을 지정하도록 할 수 있습니다.

본 논문에서는 기존의 혼합 음성에서 화자 개별 음성 데이터를 훈련한 결과보다 논문에서 제시한 (한 번 훈련한) 시청각 통합 모델이 성능이 더 우수하다는 것을 보여줍니다.

<figure>
    <img src = "https://i.imgur.com/vls977k.png">
    <figcaption>꼭 영상을 보면 좋겠습니다.</figcaption>
</figure>

## 더 읽어보기

### 데이터셋 AVSpeech

이 논문에서는 배경에 간섭 시그널이 없는 음성 클립으로 구성된 대규모 음성-영상 데이터셋을 만들었습니다.
세그먼트는 길이가 3 ~ 10 초 사이에서 다양하며, 각 클립에서 비디오 한 명이 말하는 영상입니다.

총 데이터 세트에는 대략 4700 시간 분량의 비디오 세그먼트가 포함되어 있으며 다양한 사람, 언어 및 얼굴 포즈에 걸쳐 약 150,000명의 사람이 있습니다.

파형과 분포는 아래 사진을 참고합시다.

<figure>
    <img src = "https://i.imgur.com/xPeDlah.png">
    <figcaption>만들고 사용한 데이터셋 : AVSpeech Dataset, TED가 눈에 띄네요</figcaption>
</figure>

데이터셋이 인간의 실질적인 피드백에 의존하지 않고, 많은 코퍼스를 섞기 위해 대규모의 데이터셋을 자동으로 수집했습니다. 유튜브 29만개의 영상에서 테드 및 how-to-video를 위주로 모았습니다. 이는 음질이 좋고, 1인 화자로 이루어진 영상입니다.

<figure>
    <img src = "https://i.imgur.com/WDT5vFW.png">
    <figcaption>데이터 수집 프로세스 과정 2가지</figcaption>
</figure>

다음 사진과 같이 데이터 수집 프로세스는 2가지 과정으로 이루어집니다.

1. 첫번째는 말하는 사람을 추적합니다. 이는 기존의 논문의 방법을 사용했습니다. 세그먼트의 얼굴  프레임 중 15% 이상이 누락된 경우는 모두 버렸다고 합니다. Google Cloud Vision API1를 통해 분류했다고 합니다.

2. 음성을 목소리에 맞게 정리하는 과정입니다. 간섭받지 않고, clean한 오디오를 만드는 것입니다. 여기서는 음성  SNR을 사용하여 자동으로 처리했다고 합니다. 여기서는 LibriVox 공개 오디오북 컬렉션 연설에서 교육된 모델을 사용했고, 임계치를 지정해 분류했다고 합니다. 후에 랜덤으로 100개 정도의 샘플을 듣고 심한 노이즈가 없음을 확인했습니다.

### 모델

모델은 다음과 같습니다. 모델이 참 복잡하지만 이쁘게 생겼습니다.

<figure>
    <img src = "https://i.imgur.com/UAXOF1y.png">
    <figcaption>Model Visualization</figcaption>
</figure>

### 음성 데이터

<figure class = "half">
    <img src = "https://i.imgur.com/7Ric74b.png">
    <img src = "https://i.imgur.com/SFOKL2T.png">
    <figcaption>음성 데이터</figcaption>
</figure>

### 다양한 시나리오

![시나리오](https://i.imgur.com/9NviDBF.png)

### 영상과 음성의 관계 시각화

![audio-visual](https://i.imgur.com/x3otDT3.png)

## 결론

데이터셋을 만드는 방법 그리고 재밌는 주제

읽으면서 너무 즐거운 논문이었습니다.
