---
title : "[LR#014] Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers"
category :
  - LR
tag :
  - Survey
  - Deeplearning
  - Visual Analytics
sidebar:
  nav: sidebar-youtube
use_math : true
header:
  overlay_image : "http://www.lighting-inspiration.com/wp-content/uploads/2015/08/Lighting-Inspiration.com_Rohinni-Lightpaper1.jpg"
  overlay_color : "#E0BBE4"
---

딥러닝 시각화 시리즈 2

> 이번에는 제가 전체적으로 리뷰하고 싶어, 원문과 해석이 혼합되어 있을 수 있습니다. 원문의 내용만을 원하시는 분들에게 미리 사과의 말씀을 전합니다.

이번에는 시각화 Survey논문입니다. 시각화에 대해 본격적으로 들어가기 전 기존 딥러닝 논문에서 시각화에 대한 여러 내용을 가볍게 볼 수 있을 것 같습니다.

논문 제목은 **Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers** 입니다. 2018년 1월에 나온 논문입니다.

부족한 영어 실력이라 원문의 내용을 모두 흡수하지는 못했지만 읽는 과정이 즐거웠습니다. 큰 타이틀만 봐도 좋을 것 같습니다.

- **paper** : https://arxiv.org/pdf/1801.06889v3.pdf

## 초록으로 읽기

> 요약하자면 딥러닝에서 시각적 분석(visual analytics)을 survey한 논문입니다.

딥러닝은 최근에 급속한 발전을 보였으며 이전에 생각했던 어려운 문제에 대한 최첨단 성능으로 인해 주목을 받았습니다. 그러나 deep neural netwrok(심층신경망)의 내부 복잡도(internal complexity)와 비선형 구조로 인해 이러한 모델이 이러한 성능을 달성하는 이유에 대한 근본적인 의사 결정 프로세스는 해석하기 어렵습니다.

딥러닝은 여러 도메인에 적용되고 있습니다. 그렇기에 모델을 (1)올바르게 작동 할 때, (2)실패 할 때, (3)궁극적으로 성능을 향상시키는 방법을 이해할 수있는 도구를 사용하여 사용자에게 도구(tool)을 제공하는 것이 가장 중요합니다.

심층신경망을 구축하기 위한 표준화된 툴킷들은 많은 사람의 딥러닝 접근에 도움이되었습니다.
그 중에서 visual analytics system(시각적 분석 시스템)은 모델 설명, 해석, 디버깅 및 개선을 지원하기 위해 개발되었습니다.

본 논문은 육하원칙에 입각하여 시각적 분석 시스템이 딥러닝에서 어떤 역할을 하는지 설문하여 정리 및 요약하였습니다.

논문은 연구 방향을 강조하고 연구 문제를 공개하고 결론을 맺습니다. 이 설문 조사는 시각적 분석과 딥러닝 모두에서 연구자와 실무자가 젊고 빠르게 성장하는 연구의 주요 측면을 신속하게 학습 할 수 있도록 도와줍니다.

## Introdutcion

![Overview](https://i.imgur.com/14mpudI.png)

총 6가지 질문으로 구성됩니다. 그림에서 회색 글씨가 각 주제에 대한 서브 주제입니다.

- **Why** : Why do we want to visualize deep learning?
- **Who** : Who wants to visualize deep learning?
- **What** : What can we visualize in deep learning?
- **How** : How can we visualize deep learning?
- **When** : When can we visualize deep learning?
- **Where** : Where is deep learning visualization being used?

## I. Why : WHY VISUALIZE DEEP LEARNING

왜 딥러닝에서 시각화가 필요할까요?
이에 네 가지로 설명할 수 있습니다.

- Interpretability & Explainablility (해석과 설명)
- Debugging & Improving Models (모델 디버깅과 모델 개선)
- Comparing & Selecting Models (모델 비교와 선택)
- Teaching Deep Learning Concepts (딥러닝 개념 교육)

### I.1 Interpretability & Explainablility

> 저는 Interpretation을 해석, Explaination을 설명이라고 표현합니다.

딥러닝에서 시각화가 중요한 이유는 모델이 **의사결정을 내리는 방식** 과 모델이 **학습한 내용을 알기 위해서** 입니다. 그리고 이를 알 수 있다면 모델을 더욱 신뢰할 수 있습니다.

이를 위해 다양한 논문에서 다음과 같은 표현을 사용하며 모델의 내부 복잡도에 대해 언급하고 있습니다.

- opening and peering through the black-box
- transparency
- interpretable neural network

#### I.1.1 Discordant Definitions for Interpretability (Interpretability에 대한 모호한 정의)

하지만 안타깝게 Interpretability와 Explainablility의 공식적이고, 합의된 정의가 없습니다. 단어의 정의에 관한 다양한 논의는 다음과 같습니다.

|Author|Explaination|Interpretation|
|-|-|-|
|Lipton | 모델의 메커니즘을 따로 설명하지 않아도 예측 값을 보여주는 것||
|Montavona et al.| 주어진 예제가 결정하는 데 기여한 해석 가능 도메인의 기능 모음 | 추상적 개념을 사람이 이해할 수 있는 영역으로 매핑하는 것 |
|Miller | 인간 관찰자에게 결정이나 행동을 **설명(Explaination)** 하는 것에 초점을 맞추는 것이 목표라면,이 기술들이 성공하려면 인간이 받아 들일 수있는 구조를 가져야한다   |   |
|Offert   |   | 해석(Interpretation)을 더욱 엄밀히 하기 위해서는 직감적인 고려를 통해 어디가 정확히 아닌 것인지 알아야한다. |

#### I.1.2 Interpretation as Qualitative Support for Model Evaluation in Various Application Domains

갑자기 왜 Interpretation(해석)에 대한 정의를 다루는가 싶습니다. 하지만 많은 데이터 사이언스 및 AI 프로젝트 모델의 정성적인 해석을 포함하고 있고, 이를 통해 모델의 예측과 그 작업을 보다 주장할 수 있습니다.

**결론** : 시각화는 다양한 도메인에서 이런 해석 또는 설명에 도움이 됩니다.

### I.2 Debugging & Improving Models

기계학습 모델 또는 심층 신경망을 개발하는 것은 반복적인 설계 과정입니다.

수학적인 토대가 마련되었지만, 여전히 딥러닝은 해결해야할 문제가 많습니다.

예를 들면 모델 깊이, 레이어 너비, 하이퍼파라미터 미세 조정 등이 있습니다.

이런 반복적인 실험과정을 신속하게 수행하면 좋은 성능까지 빠르게 도달할 수 있습니다. 그렇기에 효과적인 디버깅을 위해 시각적인 툴이 많이 제시되었습니다.

**결론**

1. 모델 설계는 반복적
2. 모니터링하고 디버깅 필요
3. 이를 시각화로 가능
4. 모델 설계가 빨라짐
5. 좋은 모델을 빨리 개발할 수 있음

### I.3 Comparing & Selecting Models

디버깅 및 성능 개선과 조금 다른 부분으로 모델 비교 및 선택을 설명합니다.

여기서는 디버깅이 필요없이 이미 학습된 결과에 대해서 이야기하고 있습니다.

예를 들면 단일 모델을 선택해야할 때, 원하는 좋은 결과를 찾고, 모델 간 비교하기 위해 모델의 일부분을 시각화하는 것입니다. 즉, 일반화가 잘 된 모델을 찾기 위해 사용합니다.

일부 시스템에서는 이런 모델 측정 값을 high-level로 접근하기 위해 Interactive Chart를 만들기도 합니다.

다른 프레임워크에서는 서로 다른 무작위 초기화 단계에서 성능에 미치는 영향을 파악하여 성능을 정량화하고 해석합니다.

이미지 생성에서 네트워크 아키텍처의 차이에 따른 결과를 비교하기 위해서도 시각화합니다.

모델을 비교하는 것과 마찬가지로 단일 모델에서 epoch에 따른 변화를 시각화하여 비교하기도 합니다.

### I.4 Teaching Deep Learning Concepts

비전문가에게 딥러닝 개념을 설명하기 위해서도 사용합니다. 다음과 같은 예시가 있습니다.

- Teachable Machines
- Deep Visualization Toolbox
- TensorFlow Playground
- “How to Use t-SNE Effectively"
- "Visualizing MNIST"

## II. Who : WHO USES DEEP LEARNING VISUALIZATION

누가 딥러닝 시각화를 사용할까요?
이는 총 3개의 부류로 나눌 수 있습니다.

- Model Developers & Builders (개발자)
- Model Users (사용자)
- Non-experts (비전문가)

### II.1 Model Developers & Builders

모델을 개발하는 사람은 디버깅하고 위의 목적을 위해 필요합니다.
이들을 위한 툴은 다음과 같은 툴들이 있습니다.

- TensorBoard
- DeepEyes
- Blocks
- ML-o-scope
- DGMTracker

### II.2 Model Users

딥러닝에 대한 기술적인 배경은 있으나 아직 초보인 사용자입니다.
딥러닝을 이용하여 예술을 하는 분들도 포함합니다.

- ActiVis
- LSTMVis
- Embedding Projector

### II.3 Non-experts

딥러닝에 대한 기술적인 지식이 전무할 수 있으며, 여기서의 목표는 딥러닝 컨셉에 대한 교육입니다.

- TensorFlow Playground
- ShapeShop
- ConvNetJs & TensorFlow.js

## III. What : WHAT TO VISUALIZE IN DEEP LEARNING

딥러닝에서 어떤 것을 시각화할까요?

- Computational Graph & Network Architecture (그래프 및 네트워크 아키텍처)
- Learned Model Parameters (학습된 모델 파라미터)
- Individual Computational Units (개별적 컴퓨팅 유닛)
- Neurons in High-dimensional Space (고차원 공간에서 뉴런)
- Aggregated Information (종합적 정보)

### III.1 Computational Graph & Network Architecture

가장 필요한 것은 모델 아키텍처입니다.

이는 모델이 어떻게 훈련, 테스트, 저장, 체크 포인트를 수행하는지를 포함하는 Computational Graph를 포함합니다. dataflow graph라고도 합니다.

### III.2 Learned Model Parameters

훈련 중, 또는 훈련 후 파라미터를 시각화할 수 있습니다.

- Neural Network Edge Weights
- Convolutional Filters

### III.3 Individual Computational Units

개별적인 edge가 아닌 가중치 값들도 시각화할 수 있습니다.

- Activations
- Gradients for Error Measurement

### III.4 Neurons in High-dimensional Space

피쳐를 차원이라고 생각한다면, 후에 차원 감소를 통해 피쳐를 시각화할 수 있습니다.

### III.5 Aggregated Information

다음과 같은 정보도 시각화할 수 있습니다.

- Groups of Instances
- Model Metrics

## IV. How : HOW TO VISUALIZE DEEP LEARNING

어떻게 딥러닝을 시각화할까요?

- Node-link Diagrams for Network Architectures (네트워크 구조를 위한 노드-링크 다이어그램)
- Dimensional Reduction & Scatter Plots (차원 축소, 산포도)
- Line Charts for Temporal Metrics (측정을 위한 선도표)
- Instance-based Analysis & Exploration (인스턴스 기반 분석 및 탐색)
- Interactive Experimentation
- Algorithms for Attribution & Feature Visualization


### IV.1 Node-link Diagrams for Network Architectures

가장 일반적인 모델 아키텍처 표현 방법은 Node-Link 다이어그램입니다.

- Node : Neuron
- Link : Edge Weight

Kahng et al. 에 의하면 두 가지 방법이 있습니다.

1. 연산만 표시
2. 연산과 데이터 표시

TensorBoard와 interactive dataflow graph visualization에 의해 1번이 표준으로 자리 잡았습니다. 하지만 이렇게 하면 너무 복잡한 구조가 생깁니다.

그렇기에 Wongsuphasawat et al. 은 high-degree node를 추출하여 이를 메인 그래프 외로 시각화합니다. 이는 코드 내에서 super-group을 정의할 수 있도록 합니다.

혼란을 줄이는 방법으로 각 노드에 더 많은 정보를 배치하는 것입니다. DGMTracker는 노드 안팎으로의 데이터 흐름에 대한 빠른 스냅샷을 제공합니다.

가중치와 부호는 색상 또는 링크의 두께를 사용하여 표현할 수 있습니다.

Harley는 각 계층의 convolution  window를 활성화에 따라 표현했습니다.

너무 많은 링크가 있는 경우  bi-clustering-based edge bundling technique를 사용하여 혼란을 줄일 수 있다고 합니다.

### IV.2 Dimensional Reduction & Scatter Plots

t-SNE 또는 PCA를 이용하여 차원을 줄이고 데이터를 2차원으로 표현할 수 있습니다.

### IV.3 Line Charts for Temporal Metrics

진행 상황에 따른 결과를 선도표로 표현할 수 있습니다. 훈련이 잘 이루어져있는가를 측정할 때 유용합니다.

### IV.4 Instance-based Analysis & Exploration

- Identifying & Analyzing Misclassified Instance
- Analyzing Groups of Instance

### IV.5 Interactive Experimentation

- Models Responding to User-provided Input Data
- How Hyperparameters Affect Result

### IV.6 Algorithms for Attribution & Feature Visualization

- Heatmaps for Attribution, Attention & Saliency
- Feature Visualization

## V. When : WHEN TO VISUALIZE IN THE DEEP LEARNING PROCESS

- During Training (훈련 과정 중)
- After Training (훈련이 끝나고)

### V.1 During Training

모델이 잘 훈련되고 있는지 확인하기 위해 사용합니다. metrics라고 표현되는 측정 값은 3가지 측면에서 중요합니다.

1. 모델이 배우기 시작했는가
2. 성능이 수렴하여 피크에 도달하였는가
3. 훈련 데이터셋에 과대적합된 것은 아닌가

그렇기에 많은 시각적 분석 시스템은 metric의 실시간 업데이트를 기본보기로 표시합니다.

- Deep View에서는 자체 정의 된 메트릭을 시각화하여 보여주기도 합니다. (density metric which evaluates the output feature maps.) 이를 통해 오버피팅되는지 미리 알 수 있습니다. 그럼 빠르게 프로세스를 중지할 수 있습니다.

- DeepEyes는 레이어가 안정한지 불안정한지를 식별합니다.

- Block은 모델 디자인에서 class confusion이 일어나는 부분을 시각화하여 exploit을 될 수 있는 부분을 보여줌

- DGMTracker는 다른 뉴런이 특정 실패 뉴런의 출력에 어떻게 기여 하는지를 나타내는 신용 할당(credit assignment) 알고리즘 제안



### V.2 After Training

다음과 같은 툴을 참고하면 좋을 것 같습니다. (생략)

- Embedding Projector
- Deep Visualization Toolbox
- ActiVis
- RNNVis
- LSTMVis

## VI. Where : WHERE IS DEEP LEARNING VISUALIZATION

- Application Domains & Models (응용 분야 도메인 및 모델)
- A Vibrant Research Community: Hybrid, Apace & Open-sourced (활기찬 연구 커뮤니티)

### VI.1 Application Domains & Models

다음과 같은 분야에서 시각화가 필요합니다.

- Neural machine translation (기계 번역)
- Reinforcement learning  (강화 학습)
- social good (사회적 이익)
- autonomous vehicles (자율 주행)
- medical imaging diagnostics (의료 이미지)
- urban planning (도시 게획)

그 외 CNN, RNN, GAN 같은 모델 및 방법론에서도 시각화가 필요합니다.

### VI.2 A Vibrant Research Community: Hybrid, Apace & Open-sourced

매일 출간되는 새로운 연구의 양을 소화하는 것이 어렵지만 코드로 작업을 액세스하면 재현성이 촉진되고 커뮤니티가 더 빨리 진행될 수 있습니다.

이런 오픈 소스를 더욱 효율적으로 영향력을 행사하기 위해서는 시각화를 통한 가시성을 높이는 것이 중요합니다.

## VII. RESEARCH DIRECTION & OPEN PROBLEMS

가장 중요한 부분이라고 생각합니다. 이 survey의 결론입니다.

앞으로의 연구 진행은 어떻게 이뤄져야할까요? 어떤 부분을 초점으로 두고 어떤 아이디어를 떠올릴 수 있을까요?

- Furthering Interpretability (해석과 설명)
- System & Visual Scalability (확장성)
- Design Studies for Evaluation: Utility & Usability (유용성과 편리성을 위한 설계)
- The Human Role in Interpretability (해석에서 사람의 역할)
- Social Good & Bias Detection (사회 기여 및 편향 방지)
- Protecting Against Adversarial Attacks (적대적 공격으로부터의 보호)

### VII.1 Furthering Interpretability

딥러닝에서 해석과 설명은 앞으로 더 중요해질 것입니다.

- information visualization and visual analytics communities : 새로운 시각적 표현 생성, 새로운 상호 작용 기술 개발
- AI communities : 통찰력 있는 결과 및 피처 시각화, 빠른 시각화(비용이 낮은 시각화)

### VII.2 System & Visual Scalability

시각화의 대표적인 문제로는 확장성 문제가 있습니다.

확장성 문제를 해결하기 위해서는 복잡한 데이터 흐름을 설명하기 위해 이를 단순화하는 작업이 필요합니다.

하지만 activations, embedding, 차원 감소 등의 기술은 표현해야할 포인트의 수에 따라 유용성이 제한됩니다.

모델 성능 저하 없는 시각화는 필요합니다. 특히 사용자 상호 작용을 지원하거나 화면을 렌더링하기 전에 사전 계산을 수행해야하는 시각 시스템에 중요합니다.

### VII.3 Design Studies for Evaluation: Utility & Usability

시각적 표현의 유용성 평가는 시각화 연구의 중요한 측면 중 하나입니다.
마찬가지로 배포된 시스템과 인터랙티브한 시스템의 유용성 평가 또한 중요합니다.

그렇기에 사용자와 사용자의 니즈를 파악하여 모델을 설계해야합니다

우리는 잘 연구 된 지침 [86]을 사용하여 인터페이스를 구성하고, 우선 순위를 정하는 데 도움이 될 수 있는 인터페이스 및 사용자 경험 설계자를 포함하여, 더 많은 인간-컴퓨터 상호 작용 커뮤니티 구성원을 포함시켜 향후 작업이 더 많은 이익을 얻을 수 있다고 생각합니다.

### VII.4 The Human Role in Interpretability

#### VII.4.1 Human v. Machine Understanding of the World

인간과 기계가 각각 세계를 보고 분해하는 방식의 공통점과 차이점을 이해하기 위해 노력하고 있습니다.

#### VII.4.2 Human-AI Pairing

시각화는 AI의 결과를 설명하기 위해서 사용되고, 인간은 다시 이를 이용해 최고의 퍼포먼스를 내는 모델을 설계합니다.

**Intelligence Augmentation** 라고 불리는 이 분야는 AI 결과를 통해 생각을 확장시키고 창의적인 태스크를 만들어 positive feedback이 진행되게 할 수 있습니다.

인간과 AI의 상호보완적인 구조를 목표로 삼을 수 있습니다.

### VII.5 Social Good & Bias Detection

AI를 교육 도구로 사용하며 이를 모든 이들이 누릴 수 있게 노력해야합니다. 또한 특정 편향에 대한 불이익 등을 고려하여 인간 편견이 개입되지 않게 방지하여야 합니다.

### VII.6 Protecting Against Adversarial Attacks

시각화를 이용한 Adversarial Machine Learning 공격을 방지하는 것이 하나의 미션이 될 것입니다.
