---
title : "[LR#008] GANSynth: Adversarial Neural Audio Synthesis"
category :
  - LR
tag :
  - audio
  - GAN
  - music
  - wavenet
sidebar:
  nav: sidebar-youtube
use_math : true
header:
  overlay_image : "http://www.lighting-inspiration.com/wp-content/uploads/2015/08/Lighting-Inspiration.com_Rohinni-Lightpaper1.jpg"
  overlay_color : "#E0BBE4"
---
GAN이 이미지에서 성능이 좋다며? 음성도 이미지처럼 다뤄보자!

GAN은 이미지에서 매우 좋은 성능을 내고 있습니다. 하지만 오디오에서는 여전히 힘을 못쓰고 있습니다. 그 이유는 그만큼 사람이 이상한 부분을 인지하기 쉽기 때문입니다.

그렇다면 오디오를 이미지처럼 변환하고, 변환한 이미지를 다시 오디오로 변환한다면...?

이 아이디어를 구현한 Magenta의 GANSynth를 소개합니다. 논문 제목은 **GANSynth: Adversarial Neural Audio Synthesis** 입니다.
2019년 2월에 나온 논문입니다.

- paper : [https://openreview.net/pdf?id=H1xQVn09FX](https://openreview.net/pdf?id=H1xQVn09FX)
- website : [https://magenta.tensorflow.org/gansynth](https://magenta.tensorflow.org/gansynth)
- github : [https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth](https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth)

웹사이트로 들어가시면 코랩으로도 공유가 되어 있으니 궁금하신 분은 보면 좋을 것 같습니다.

@하우론 님의 블로그에서 : [[학부생의 딥러닝] GANs \| GANSynth : Adversarial Neural Audio Synthesis](https://haawron.tistory.com/22) 글을 많이 참고했습니다.


## 초록으로 읽기

![result](https://i.imgur.com/61W4u0v.png)

사람은 오디오에서 글로벌 구조와 미세한 파형 일관성에 민감합니다.
그렇기에 효율적인 오디오 합성은 본질적으로 어려운 머신러닝 과제입니다.

WaveNet과 같은 자동 회귀(Autoregressive) 모델은 비싼 전역 잠재 구조(global latent structure)와 느린 반복 샘플링을 통해 로컬 구조를 모델링합니다.

반면 Generative Adversarial Networks(GAN)는 전역 잠재 구조 조건 설정과 효율적인 병렬 샘플링을 갖추고 있지만, 오디오 waveform에서 지역-일관성(local-coherent)을 가지기 힘듭니다.

여기서는 GAN이 스펙트럼 도메인에서 충분한 주파수 해상도의 로그 크기와 순간 주파수(log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain)를 사용해 높은 정확도와 지역-일관성을 가진 오디오를 생성 할 수 있음을 보여줍니다.

NSynth Dataset을 사용한 광범위한 경험적 조사를 통해 본 논문의 GAN이용 모델이 WaveNet보다 우수한 성능을 보이고, 오디오를 보다 빠르고 효율적으로 생성 할 수 있음을 입증합니다.

### 결론

오디오의 spectrogram을 생성하여 GAN으로 spectrogram을 구성하니 좋고, 빠른 성능으로 오디오 합성을 할 수 있었다.


## 더 읽어보기

### How Does it work?

GANSynth는 [Progressive GAN](https://arxiv.org/abs/1710.10196)을 사용하여 단일 벡터를 full sound까지 업샘플링합니다.
비슷한 [이전 논문](https://arxiv.org/abs/1802.04208)에서 주기적인 신호에서는 어렵다는 것을 발견했습니다.

즉, 주기성을 잘 이용하는 것이 이 작업의 포인트입니다. 이해를 돕기 위해 아래 사진을 봅시다.

![waverform](https://i.imgur.com/MMUeLlc.png)

위 사진에서 빨강-노랑 곡선은 주기적인 신호이고, 검정 점은 각 주기의 시작점입니다.

위 검은 점선과 같이 주기적 프레임으로 이 신호를 처리하면 주기의 시작점은 모두 변경됩니다.

strided convolution의 경우, 모든 위상 순열을 학습해야하고 이는 매우 비효율적입니다. 심지어 시간이 지날수록 이 위상차(원래 주기와 차이, 그림에서 검은선)는 매우 심해질 것입니다. 이를 좀 더 그래프로 봅시다.

![phase](https://i.imgur.com/XCl57fY.png)

위상차는 히스토그램으로 확인하면 다음과 같습니다.

- 처음 위상차는 애매한 주기성을 가집니다(노랑)
- 여기서 Unwrapped phase라는 과정, 즉 mod $2\pi$를 해주면 선형으로 만들 수 있습니다. (주황)
- 그리고 이 차이는 일정합니다. 이를 instantaneous frequency(이하 IF)라고 합니다. (빨강) (**시간에 따른 Unwrapped phase의 미분값이 일정하다** 가 더 정확한 표현입니다.)

![steps](https://i.imgur.com/rdJ045M.png)

이제 좀 더 spectrogram 관점에서 봅시다.

- 목소리나 악기 소리는 국소적으로 주기적인 성질을 가지고 있습니다. STFT(Short-Time Fourier Transform)로 magnitude만 sampling하면 ground truth 신호와 위상이 어긋나는 경우가 생깁니다.
- 이를 Unwrapped Phase로 변환하면 어느정도 더 주기성이 보이고
- 이를 미분하면 더 명확한 차이가 보이기 시작합니다.
- 이를 로그수준으로 바꾸면 명확하게 구분할 수 있습니다.

이제 이 spectrogram을 생성하는 작업을 GAN으로 하면 더 좋은 결과를 낼 수 있는 것입니다.
즉, `오디오 -> 이미지 -> 생성이미지 -> 생성오디오`의 과정입니다.

### 논문 포인트

- Waveform이 아닌 log-magnitude spectrogram이랑 phase를 직접 GANs으로 만들었더니 좀 더 실제에 가까운 소리를 얻을 수 있었습니다.

- IF spectra를 예측하면 phase예측보다 더 좋은 결과를 얻을 수 있다.

- 저음 부분의 STFT 필터들이 겹치지 않도록 하는 것은 중요합니다. Mel Frequency scale로 바꾸거나 STFT frame size를 늘리는 것으로 해결할 수 있다. (음들이 대부분 저음에서 많이 겹치기 때문)

- WaveNet보다 54000배 빠르게 sample을 만들 수 있습니다.

- Global Condition에 따라 latent and pitch vector 제한을 통해 더 부드러운 음색의 보간이 가능합니다. 음색을 유지하며 피치를 바꿀 수도 있었습니다.

### Dataset

데이터셋은 Nsynth dataset을 사용했습니다.

- 300000개
- 1000개의 서로 다른 독립된 악기 음성
- 다양한 피치, 속도, 악기, 품질이 잘 구성되어 있음
- 각기 4초, 16000Hz (=64000 차원)

평가(human evaluation)를 위해 데이터셋은 MIDI 24-84 (32~1000Hz)에  제한을 뒀습니다. 그렇게 거의 25%가량인 70379개의 데이터셋만 사용했다고 합니다.
그 결과 현악기, 금관악기, 목관악기, 맬릿 악기가 위주로 남았다고 합니다.

> 음알못인 저는 대체 어떤 악기가 걸러진건지 전 모르겠네요.

훈련과 테스트는 셔플 후에 80/20으로 나눴습니다. (후에 분류기로도 사용가능)
데이터셋에 대한 더 많은 정보는 다음 링크에서 확인할 수 있습니다.

- [The NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth)

### Model

생성자와 감별자의 모델은 다음과 같습니다.

<figure class = "half">
    <img src ="https://i.imgur.com/uN6VyDG.png">
    <img src ="https://i.imgur.com/WG06E4G.png">
    <figcaption>생성자와 감별자</figcaption>
</figure>

여기서 다음과 같은 variation을 통해 성능을 향상시키는 다양한 방법을 시도했다고 합니다.

- Phase Normalization
- Unwrapped
- (1024,1024,2) image input
- mel freq scale

### 그 외

데이터셋으로부터 STFT를 사용해 (256, 512, 2) 스펙트로그램 이미지를 얻었다. 256은 stride, 512는 freq bins, 2는 mag와 phase를 의미합니다.
G에서  tanh를 이용하기 때문에 magnitude 이미지들의 값들에 log를 취하고 -1~1로 normalize 했습니다.

Baseline으로는 audio GAN계열의 WaveGAN와 autoregressive 계열의 WaveNet을 사용했습니다.



## Result & Conclusion

이런 부분에서 좋은 성과를 내었습니다.

- Phase Coherence
- Interpolation
- Consistent Timbre Across Pitch
- Fast Generation

<figure class = "third">
    <img src ="https://i.imgur.com/61W4u0v.png">
    <img src ="https://i.imgur.com/EIeckMC.jpg">
    <img src ="https://i.imgur.com/9ms7IGY.jpg">
    <figcaption>향상된 결과 예시</figcaption>
</figure>

병렬적으로 연산이 가능하므로 GPU의 효율적인 사용이 가능합니다.

4초 음원 하나를 만드는데, Wavenet이 1077초 걸렸다면 본 모델은 20ms만에 생성이 가능했습니다. (Titan X 기준)

## 추가로 볼 수 있는 점

- Spherical Gaussian Latent Vector : 논문에서 사용한 구-가우시안 잠재 벡터
- 평가 지표 : 다른 GAN논문과 비슷하게 AMT 사용

아직 음성에 대한 처리에 깊은 이해가 부족해 논문이 가지는 특징을 더 깊게 이해하지 못한 것 같아 아쉽습니다. 오디오 관련 공부를 더 깊게 해야겠습니다.

## Reference

- magenta blog : [GANSynth: Making music with GANs](https://magenta.tensorflow.org/gansynth)

개발팀 기술 블로그입니다. 이 연구에 관련된 다양한 정보를 확인할 수 있습니다.

- @하우론 : [[학부생의 딥러닝] GANs \| GANSynth : Adversarial Neural Audio Synthesis](https://haawron.tistory.com/22)

이미 정리하신 분이 있어 많이 참고했습니다. 저보다 훨씬 구체적으로 분석한 자료입니다.
