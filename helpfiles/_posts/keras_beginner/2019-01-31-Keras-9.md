---
title : \[Keras Study\] 9장. 결론
category :
  - ML
tag :
  - keras
  - deep-learning
  - AI
  - machine learning
  - colab
  - 케라스
  - 머신러닝
  - 딥러닝
  - 입문
  - subinium
  - 소스코드
sidebar:
  nav: sidebar-keras

use_math : true

header:
  teaser : /assets/images/category/ml.jpg
  overlay_image : /assets/images/category/ml.jpg

---

드디어 책 1회독 완성!

본 문서는 [케라스 창시자에게 배우는 딥러닝] 책을 기반으로 하고 있으며, subinium(본인)이 정리하고 추가한 내용입니다. 생략된 부분과 추가된 부분이 있으니 추가/수정하면 좋을 것 같은 부분은 댓글로 이야기해주시면 감사하겠습니다.

책 정주행이 드디어 코 앞입니다. 지금까지 한 내용의 총정리와 딥러닝에 대한 앞으로의 공부 방향을 생각하면서 보면 좋을 것 같습니다.

## 9.1 핵심 개념 리뷰

이 절은 책의 핵심 내용에 대한 간단한 정리입니다.

### 9.1.1 AI를 위한 여러 방법

딥러닝은 AI, 머신 러닝과 동의어가 아닙니다. **인공 지능** 은 역사가 깊고 광범위한 분야로 일반적으로 인지 과정을 자동화하기 위한 모든 방법을 말합니다.
즉 사고의 자동화를 목표로 합니다.

**머신 러닝** 은 훈련 데이터를 사용하여 자동으로 프로그램(모델)을 개발하는 AI의 특정 하위 분야입니다.
데이터를 프로그램으로 바꾸는 과정을 **학습** 이라고 부릅니다.

**딥러닝** 은 머신 러닝의 여러 종류 중 하나입니다. 기하학적 변환 함수들이 번갈아 가며 연속적으로 길게 연결된 모델입니다.
이 연산들은 **층** 이란 모듈을 구성합니다. 층은 학습하는 동안 **가중치** 파라미터를 가집니다.

딥러닝은 머신 러닝 방법 중에서도 성공한 분야입니다. 그 이유는 다음 절에서 설명합니다.

### 9.1.2 머신 러닝 분야에서 딥러닝이 특별한 이유

딥러닝은 역사적으로 컴퓨터에서 매우 어렵다고 인식된 다양한 종류의 문제에서 큰 성과를 거두었습니다.
특히 이미지, 비디오, 사운드 등에서 유용한 정보를 추출하는 기계 인지 분야입니다.
충분한 훈련 데이터가 주어지면 사람이 인식하는 거의 모든 것을 데이터에서 추출할 수 있습니다.

예전에 없던 기술 성공으로 딥러닝은 AI의 세 번째 붐을 일으켰습니다. 제가 이 글을 쓰는 순간은 아직 AI가 핫한 시기입니다.
거품이라고 생각할 수 있지만, 이전 붐과는 확연히 다른 점이 있습니다.
딥러닝이 많은 IT 기업에 엄청난 비즈니스 가치를 제공하고 있다는 점입니다.
사람 수준의 음성 인식과 이미지 분류, 스마트 비서, 크게 향상된 기계 번역 등이 있습니다.

저자는 딥러닝에 대해 10년간 더 이상 기술이 발전되지 않더라도 현재 알고리즘을 가능한 모든 문제에 적용하면 대부분 산업에서 중요한 혁신을 이룰 것이라고 예측하고 있습니다.
자원과 인력에 대한 기하급수적인 투자로 인해 엄청나게 빠른 속도로 발전하고 있습니다. (GPU, 모델 등 다양한 방면)
다음 10년간 가능한 모든 분야에 딥러닝이 적용될 것이라 예측합니다.

### 9.1.3 딥러닝에 대하여

딥러닝에서 가장 놀라운 점은 단순함에 있습니다.
경사 하강법으로 파라미터 기반의 단순한 모델 훈련을 통한 기계 인지 문제의 해결은 10년전만 해도 하지 못했고, 예상하지 못한 기술입니다.

딥러닝에서 모든 것은 벡터입니다. 모두가 기하학적 공간에 놓인 하나의 포인트입니다. 모델 입력과 타깃이 먼저 벡터로 바뀌어야 합니다.
딥러닝 모델의 각 층은 데이터를 간단한 기하학적 변환을 수행하여 통과 시킵니다.
단순환 변환의 연결로 이루어진 이 변환은 입력 공간을 타깃 공간으로 한 번에 하나의 포인트로 매핑합니다.
변환을 결정하는 파라미터가 가중치입니다.
경사하강법을 사용하기 위해 변환에 관한 식은 항상 미분 가능해야 합니다.
이를 뭉쳐진 종이 공을 펼치는 것과 유사하다고 묘사하고는 합니다.

모든 것은 하나의 핵심 아이디어에서 출발합니다. 의미는 어떤 두 가지 사이 관계에서 유도되며, 이는 거리 함수로 측정 가능하다는 것입니다.
뇌가 기하학적 공간을 통해 의미를 해석하는지 여부는 완전히 다른 이야기입니다.
지능을 표현하기 위해서는 그래프 같은 자료구조가 유용하고, **신경망** 이라는 이름은 그래프를 이용하여 의미를 인코딩하려는 시도에서 비롯되었습니다.
실제로 신경망이라는 이름은 크게 상관이 없고, **층 기반 표현 학습**, **계층적 표현 학습** 등이 더 적절합니다.

### 9.1.4 핵심 기술

기술 혁명은 뛰어난 발명 하나가 아닌 여러 요인의 누적에서 비롯됩니다. 딥러닝은 다음과 같은 요인이 있습니다.

- 알고리즘 혁신이 늘어납니다. 역전파를 개발한 후 20년이 걸렸지만, 2012년 이후 많은 연구자가 참여하며 빠른 속도로 발전하고 있습니다.
- 지각에 관련된 많은 양의 데이터를 사용할 수 있습니다. 무어의 법칙이 적용된 저장 매체와 ImageNet 등의 프로젝트등이 있습니다.
- 고성능 병렬 컴퓨터 하드웨어를 값싸게 사용할 수 있습니다. (NVIDIA의 GPU)
- 이런 컴퓨팅 파워를 활용할 수 있는 다양한 소프트웨어 스택이 마련되었습니다.

미래에는 딥러닝이 오늘날 웹 기술처럼 모든 개발자들의 도구 상자에 포함된 하나의 도구가 될 것입니다.

### 9.1.5 일반적인 머신 러닝 작업 흐름

4장에서 소개한 전형적인 머신 러닝 작업 흐름입니다.

1. 문제 정의 : 데이터 가용 범위, 데이터 양, 레이블 작업
2. 목표 달성 측정을 위해 신뢰할 수 있는 방법 : 예측 정확도, 다양한 지표
3. 평가를 위한 검증 과정 준비
4. 데이터 벡터화 및 전처리
5. 상식 수준 기본 모델보다 나은 첫 모델 만들기
6. 하이퍼파라미터 튜닝 및 규제를 통한 개선
7. 검증 세트에 대한 과대적합 주의

### 9.1.6 주요 네트워크 구조

**완전 연결 네트워크, 합성곱 네트워크, 순환 네트워크** 이 세 종류 네트워크 구조는 이전 글을 읽었다면 익숙할 겁니다.
각 네트워크 종류는 특정 입력 형식을 의미합니다. 네트워크 구조는 데이터의 구조에 대한 가정을 담고 있습니다.
좋은 모델을 탐색하기 위한 **가설 공간** 이 됩니다.

이런 다양한 네트워크 종류는 더 큰 다중 네트워크를 만들기 위해 연결될 수 있습니다.
다양한 입력과 적절한 네트워크 구조 사이의 관계는 다음과 같이 정리할 수 있습니다.

- **벡터 데이터** : 완전 연결 네트워크(Desnse 층)
- **이미지 데이터** : 2D 컨브넷
- **사운드 데이터** : 1D 컨브넷(권장)이나 RNN
- **텍스트 데이터** : 1D 컨브넷(권장)이나 RNN
- **시계열 데이터** : RNN(권장)이나 1D 컨브넷
- **다른 종류의 시퀀스 데이터** : 1D 컨브넷이나 RNN이지만 순서에 상관이 있다면 RNN
- **비디오 데이터** : 3D 컨브넷 또는 2D 컨브넷 + RNN or 1D 컨브넷 조합
- **볼륨을 가진 데이터** : 3D 컨브넷

각 네트워크 구조의 특징을 간략하게 살펴봅시다.

#### 완전 연결 네트워크

완전 연결 네트워크는 벡터 데이터를 처리하는 Dense 층을 쌓은 것입니다. 입력 특성에 대한 특별한 가정을 두지 않습니다.
한 Dense 층의 유닛이 다른 층의 모든 유닛과 연결되어 있기 때문에 **완전 연결** 이라고 부릅니다.
층은 모든 입력 특성 간의 관계를 매핑합니다.

**이진 분류** 를 수행하려면 마지막 Dense 층이 하나의 유닛을 가져야 하고 시그모이드 활성화 함수를 사용해야 합니다.
손실은 `binary_crossentropy`를 사용합니다. 타깃은 0또는 1이 됩니다.

**단일 레이블 다중 분류** 를 수행하려면 마지막 Dense 층이 클래스 개수만큼 유닛을 가져야 하고 softmax 활성화 함수를 사용해야 합니다.
타깃을 원-핫 인코딩한다면 `categorical_crossentropy`를 사용하고, 타깃이 정수라면 `sparse_categorical_crossentropy`를 손실로 사용합니다.

**다중 레이블 다중 분류** 를 수행하려면 마지막 Dense 층이 클래스 개수만큼 유닛을 가져야 하고 시그모이드 활성화 함수를 사용해야 합니다.
손실로는 `binary_crossentropy`를 사용합니다. 타깃은 k-핫 인코딩되어야 합니다.

연속된 값을 가진 벡터에 대해 **회귀** 를 수행하려면 마지막 Dense 층이 예측하려는 값의 개수만큼 유닛을 가져야 하고 활성화 함수는 사용하지 않습니다.
여러 가지 손실을 사용할 수 있습니다. `mse`나 `mae`를 가장 많이 사용합니다.

#### 컨브넷

합성곱 층은 입력 텐서의 여러 위치에 동일한 기하학적 변환을 적용하여 공간 방향의 지역 패턴을 찾습니다.
이는 **이동 불변성** 을 가진 표현을 만들어 합성곱 층을 데이터 효율적으로 만들고 모듈화합니다.
이는 어느 차원의 공간에서도 적용가능합니다.

**컨브넷** 또는 **합성곱 네트워크** 는 합성곱과 최대 풀링 층이 쌓여 구성됩니다.
풀링 층은 공간 방향으로 데이터를 다운 샘플링합니다. 특성 맵의 수가 증가함에 따라 적절한 크기로 특성 맵을 유지하여 후속 합성곱 층이 입력에서 더 큰 부분을 볼 수 있게 합니다.
컨븐넷은 Flatten 연산이나 전역 풀링 층으로 끝나는 경우가 많습니다.
이 층은 공간 특성 맵을 벡터로 변환하고 그 뒤에 분류나 회귀를 위한 Dense 층이 이어집니다.

일반적인 합성곱이 동일하지만 더 빠르고 효율적으로 표현을 학습하는 **깊이별 분할 합성곱** 으로 대부분 바뀔 것 입니다. (`SeparableConv2D`)

#### RNN

**순환 신경망(RNN)** 은 한 번에 하나의 타임스텝씩 입력 시퀀스를 처리하고 이 과정 동안 **상태** 를 유지합니다. (상태는 하나의 벡터이거나 벡터의 집합입니다.)
시간 축을 따라 이동 불변성이 없는 패턴을 가진 시퀀스라면 1D 컨브넷 대신 사용하는 것이 바람직합니다.

케라스에는 3개의 RNN층이 있습니다. `SimpleRNN`, `GRU`, `LSTM`입니다.
대부분은 마지막 두 개만 사용하고, 필요에 따라 선택합니다.

여러개의 RNN 층을 겹겹이 쌓으려면 마지막 층 이전의 모든 층은 전체 시퀀스를 출력해야 합니다.
추가적인 RNN 층을 쌓지 않는다면 전체 시퀀스 정보가 담긴 마지막 출력만 반환하는 것이 일반적입니다.

### 9.1.7 딥러닝의 가능성

딥러닝으로는 다양한 작업을 할 수 있습니다.
이는 굳이 정리할 필요는 없을 것 같아 제외했습니다.

**생략**

## 9.2 딥러닝의 한계

딥러닝으루 구현할 수 있는 애플리케이션 영역은 거의 무한합니다. 아직은 레이블링한 데이터가 아주 많더라도 어려운 어플리케이션이 많습니다.
일반적으로 프로그래밍처럼 논증이 필요하거나, 장기 계획을 세워 과학적 방법을 적용하거나, 알고리즘을 사용하여 데이터를 조작하는 일은 주입하는 데이터의 양에 상관없이 딥러닝 모델로는 달성할 수 없는 영역입니다.

딥러닝 모델은 한 벡터 공간을 다른 벡터 공간으로 매핑하기 위해 단순하고 연속된 기하학적 변환을 연결한 것이기 때문입니다.
층을 추가하고 더 많은 훈련 데이터를 사용하여 현재 딥러닝 기술을 증대하는 것은 이 이슈를 표면적으로 조금 완화시킬 뿐입니다.
딥러닝 모델이 표현할 수 있는 한계가 있고 대부분의 학습 대상 프로그램은 데이터 매니폴드에 대한 연속된 기하학적 변환으로 나타낼 수 없는 근본적인 문제를 해결하지 못합니다.

### 9.2.1 머신 러닝 모델의 의인화 위험

현대 AI에서 오는 실제 위험은 딥러닝 모델이 하는 일을 잘 이해하지 못하고 그 능력을 과대 평가하는 데서 옵니다.
딥러닝 모델이 상황을 이해하고 프로그램을 실행한다고 믿는 것입니다.
하지만 모델이 잘못 분류하도록 고안된 **적대적인 샘플(adversarial example)** 을 딥러닝 네트워크에 주입할 때 이런 문제가 두드러집니다.
원하는 결과를 위해 특정 분류에 필요한 그래디언트를 더 하면 잘못된 분류를 유도할 수 있습니다.

즉 딥러닝 모델은 입력을 이해하지 못합니다. 인간의 지각 경험을 통해 형성된 가짜 지각임을 인지해야 합니다.

### 9.2.2 지역 일반화 vs. 궁극 일반화

딥러닝 모델이 수행하는 입력-출력 사이의 간단한 기하학적 변환과 사람이 생각하고 배우는 방식 사이에는 근본적인 차이가 있습니다.

사람은 심층 네트워크 또는 자극과 반응을 즉각 매핑하는 것 이상을 수행합니다. 현재 상황과 자기 자신, 다른 사람에 대한 복잡하고 추상적인 모델을 구성합니다.
이 모델을 사용하여 미래 가능성을 예측하고 장기 계획을 세웁니다. 우리는 경험하지 못한 어떤 것을 표현하기 위해 알고 있는 개념을 합칩니다.
가상의 일을 다루는 이런 능력은 **추상** 과 **추론** 을 통해 우리 마음속에 있는 모델의 공간을 직접 경험할 수 있는 것 이상으로 확장합니다.
이를 **궁극 일반화(extreme generalization)** 이라고 합니다.

하지만 심층 신경망은 **지역 일반화(local generalization)** 를 합니다.
입력-출력 매핑은 새로운 데이터가 훈련할 때 보았던 것과 조금만 달라도 납득할만한 결과를 내지 못합니다.

![일반화](https://i.pinimg.com/originals/63/de/f3/63def395b514efcdbbed242a47b2a94d.png)

기계 인지에 대한 발전 수준의 AI는 아직 멀었습니다. 새로운 환경에 적용할 수 있는 AI의 개발은 아직 어렵습니다.

### 9.2.3 정리

***생략***

## 9.3 딥러닝의 미래

딥러닝의 작동 방식, 한계, 연구 분야의 현재 상태를 알고 있다면 중/단기적으로 어디로 향하고 있는지 예측할 수 있을까요?
그런 예측은 할 수 없습니다. 세상은 어떻게 변할지 모르니까요. :-)
하지만 다음과 같은 방향의 미래를 기대할 수 있습니다.

- 모델은 범용 목적의 컴퓨터 프로그램에 가까워질 것입니다. 현재의 미분 가능한 층보다 더 뛰어난 모듈로 만들어질 것입니다. 이것이 현재 모델의 근본적인 약점인 부족한 추상과 추론을 얻는 방법이 될 것입니다.
- 위 사항을 가능하게 만드는 새로운 형태의 학습 덕택에 미분 가능한 변환에서 탈피한 모델이 등장할 것입니다.
- 엔지니어는 모델에 덜 관여할 것입니다. 끝도 없이 하이퍼파라미터를 튜닝하는 것은 기계의 역할이 될 것입니다.
- 재사용 가능하고 모듈화된 프로그램 서브루틴을 사용한 메타러닝 시스템처럼 더 훌륭하고 체계적으로 이전에 학습한 특성과 구조를 재사용할 것입니다.

지도 학습 뿐만 아니라 비지도 학습, 자기 지도 학습, 강화 학습을 포함하여 어느 형태의 머신 러닝 형태에도 적용될 것입니다.

### 9.3.1 프로그램 같은 모델

머신 러닝 분야에서 기대하는 필수적인 변화는 순수한 **패턴 인식** 을 수행하고 **지역 일반화** 만 얻을 수 있는 모델을 탈피하는 것 입니다. 추상과 추론을 통해 **궁극 일반화** 를 달성할 수 있는 모델입니다. 기본 형태의 추론을 할 수 있는 현재 AI 프로그램은 모두 프로그래머가 하드코딩한 결과입니다. (몬테 카를로 트리 탐색 등) 특정 서브 모듈만 데이터에서 학습합니다.
미래에는 이런 AI 시스템이 사람의 개입 없이 완전하게 학습될 것입니다.

관련되어 크게 성장할 것으로 예상되는 분야는 **프로그램 합성(program synthesis)** 입니다.
특히 신경망 프로그램 합성입니다. 프로그램 합성은 탐색 알고리즘을 사용해서 거대한 프로그래밍 가능 공간을 탐색하여 자동으로 간단한 프로그래밍을 합성합니다. 프로그램이 입력-출력 쌍으로 제공되는 필요 사양을 만족시키면 탐색이 중지됩니다.
이산적인 탐색 과정을 통해 소스 코드를 생성하는 것입니다.

몇 년 안에 새로운 관심을 불러일으킬 것으로 예상됩니다. 딥러닝과 프로그램 합성을 아우르는 분야가 등장할 것입니다.
범용 프로그래밍 언어로 프로그램을 생성하는 대신에 for 루프 등 다양한 알고리즘 요소로 무장한 신경망을 생성할 것입니다.

### 9.3.2 역전파와 미분 가능 층을 넘어서

머신 러닝 모델이 좀 더 프로그램처럼 된다면 더 이상 미분 가능할 필요가 없습니다.
여전히 서브루틴으로 연속된 기하학적 변환 층을 사용하겠지만 전체 모델은 그렇지 않을 것입니다.
미분 가능하지 않은 시스템을 효율적으로 훈련할 방법을 찾아야 합니다.
현재는 유전 알고리즘, 진화 전략, 일부 강화 학습, ADMM 같은 방법이 있습니다.(전 하나도 모르는군요. :-()

경사 하강법이 사라지는 것이 아닙니다. 그래디언트 정보는 미분 가능한 파라미터 모델으 최적화하는 데 언제나 유용할 것입니다.
앞으로 모델은 단순한 미분 가능한 파라미터 모델보다 점점 더 발전할 것입니다. 따라서 자동화된 개발은 역전파 이상이 필요합니다.

역전파는 end-to-end로 연속된 변환을 학습하는 데 뛰어나지만 심층 네트워크의 모듈성을 십분 활용하지 않기 때문에 계산 효율성이 떨어집니다. 효율성을 위한 보편적인 방법으로는 계층화와 모듈화가 있습니다.
동기화 매커니즘을 추가하여 훈련 모듈을 독립적으로 분리하면 역전파를 좀 더 효율적으로 수행할 수 있습니다.

### 9.3.3 자동화된 머신 러닝

현재는 딥러닝 엔지니어의 업무 대부분은 하이퍼파라미터 튜닝입니다.
전처리의 경우는 데이터에 대한 고차원적 이해가 필요하기에 AI가 돕기는 힘들지만, 그에 반한 하이퍼파라미터 튜닝은 탐색이므로 비교적 쉽습니다.
강화 학습이나 유전 알고리즘을 사용하여 아예 처음부터 적절한 구조를 학습할 수도 있습니다.
AutoML은 언젠가 모델과 가중치를 한 번에 학습하게 될 것입니다.

### 9.3.4 영구 학습과 모듈화된 서브루틴 재사용

모델이 프로그램과 더욱 비슷해지고, 그 양이 많아짐에 따라 모델의 **프로그램 서브루틴** 을 재사용하기 시작할 것입니다.

### 9.3.5 장기 비전

장기 비전을 요약하면 다음과 같습니다.

- 모델은 더 프로그램 같아질 것입니다.
- 알고리즘 모듈과 기하학적 모듈의 혼합이 이뤄질 것입니다.
- 하드코딩 대신 자동으로 만들어질 것입니다.
- 전체 공통 라이브러리와 이와 연계된 모델 성장 시스템은 사람과 비슷한 궁극 일반화의 어떤 형태를 달성할 수 있을 것입니다.
- 영구 학습 모델 성장 시스템을 **인공 일반 지능(AGI)** 으로 생각할 수 있습니다.

## 9.4 빠른 변화에 뒤처지지 않기

딥러닝은 빠른 속도로 발전하고 있습니다. 다음은 그런 딥러닝의 세계에서 계속 기술을 연마하고 앞길을 넗힐 수 있는 방법입니다.

### 9.4.1 캐글의 실전 문제로 연습하기

실전 경험에 대한 효과적인 한 가지 방법은 [캐글](https://www.kaggle.com)에서 머신 러닝 경연 대회에 직접 참여하는 것입니다.
캐글에는 많은 데이터 과학 경연 대회가 존재하고 실전 경험을 쌓을 수 있습니다.
상위 입상자에게는 상금이 주어지기도 합니다.

### 9.4.2 아카이브(arXiv)를 통해 최신 논문 읽기

딥러닝 연구 분야는 완전히 오픈되어 있습니다. 논문을 쓰면 바로 공개되어 누구나 읽을 수 있습니다.
[아카이브](https://arxiv.org)는 논문 사전 출판 공개 서버입니다.
학회 승인을 기다리지 않고 연구한 것을 주장하고 선점하는데 유용합니다. 연구 속도가 빠르고 경쟁이 치열하기 때문에 이런 것이 필수적입니다.

단점은 많은 양이 계속 포스팅되기에 좋은 논문을 구별하기 힘듭니다.
그렇기에 원하는 저자별로 [구글 스칼라](https://scholar.google.com)을 이용해 검색하는 것도 좋습니다.

### 9.4.3 케라스 생태계 탐험하기

현재 많은 커뮤니티가 활성화되어 있고, 많은 분들이 이에 대해 좋은 내용을 포스팅하고 있습니다.
AI korea, Tensorflow Korea, Keras Korea 등 페이스북 다양한 커뮤니티가 있습니다.
케라스의 경우에는 @tykimos 님의 [블로그](https://tykimos.github.io/index.html) 같이 좋은 블로그도 있습니다.

## 9.5 맺음말

이제 본격적인 공부를 또 다시 시작해봅시다!!
지금까지 subinium의 Deep Learning with Python 정리였습니다.
